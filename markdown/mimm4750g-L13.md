# MIMM4750G
## Bayesian inference
<img src="https://imgs.xkcd.com/comics/frequentists_vs_bayesians.png" width="325px"/>

---

# Likelihood

* Recall that likelihood is focusing on the probability as a function of the model parameters (hypothesis) given the data.
* For a given data set, we want to find the parameters that maximize the likelihood of our model.
* This is quite powerful! 

---

# Objections to maximum likelihood

* The maximum likelihood estimate (MLE) is a single combination of parameter values.
* If the likelihood function is "rugged", then there may be many parameter values that are about as good as the MLE!

---

# Being Bayesian

* A Bayesian would object to relying on a single estimate
* It is more robust to refer to the *distribution* of parameters that are supported by the data.
* A Bayesian would also object to the assumption that the experiment is completely objective.
* The design of an experiment is shaped by an investigator's subjective expectations about the outcome.

---

# Rev. Thomas Bayes

<table>
  <tr>
    <td>
      <ul>
        <li>A Presbyterian minister in 17th century England.</li>
        <li>"An Essay towards solving a Problem in the Doctrine of Chances" was published two years after his death.</li>
        <li>Addressed a hypothetical gambling problem proposed by [Abraham de Moivre](https://en.wikipedia.org/wiki/Abraham_de_Moivre):</li>
        <img src="/img/cards.png">
      </ul>
    </td>
    <td width="30%">
      <img src="https://upload.wikimedia.org/wikipedia/commons/d/d4/Thomas_Bayes.gif"/>
      <small>This may be a portrait of Thomas Bayes, but no one is really sure.  We use it anyhow.</small>
    </td>
  </tr>
</table>

---

# Conditional probability

* We are used to thinking about the probability of an outcome, $P(D)$.
* This implicitly involves some model (hypothesis), $P(D|H)$.
* We say that $P(D|H)$ is "the probability of the data, conditional on the hypothesis being true".
<img src="/img/conditional.png" width="500px"/>

---

# Joint probability

* We can calculate the **joint probability** that both D and H are true:
  <img src="/img/conditional2.png"/>
  
$$
P(D,H) = P(D|H) \times P(H)
$$

* It's perfectly valid to swap D and H!

$$
P(D,H) = P(H|D) \times P(D)
$$

---

# Bayes' theorem

* This leads to something wonderful:

![](/img/bayes.png)

